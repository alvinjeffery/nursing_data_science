{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Much of this content can be attributed to the work of Chris Fonnesbeck with source data found at: https://github.com/fonnesbeck/Bios8366"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Modeling & Analytics Techniques\n",
    "\n",
    "### Alvin D. Jeffery, PhD, RN & Lisianne Pruinelli, PhD, RN\n",
    "\n",
    "#### 2018 Nursing Knowledge: Big Data Science Pre-Conference\n",
    "\n",
    "#### 6/13/18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Objectives:**  \n",
    "1. Describe at least 3 modeling/machine learning techniques used in biomedical data science.  \n",
    "2. Develop a machine learning model for predicting a healthcare outcome.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## What is Machine Learning (ML)?\n",
    "\n",
    "Machine Learning (ML) is about coding programs that automatically adjust their performance from exposure to information encoded in data. This learning is achieved via **tunable parameters** that are automatically adjusted according to performance criteria.\n",
    "\n",
    "Machine Learning can be considered a subfield of Artificial Intelligence (AI).\n",
    "\n",
    "There are three major classes of ML:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Supervised learning**: Algorithms which learn from a training set of *labeled* examples (exemplars) to generalize to the set of all possible inputs.  \n",
    "Examples: regression, support vector machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Unsupervised learning**: Algorithms which learn from a training set of *unlableled* examples, using the features of the inputs to categorize inputs together according to some statistical criteria.  \n",
    "Examples: k-means clustering, kernel density estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Reinforcement learning**: Algorithms that learn via reinforcement from a *critic* that provides information on the quality of a solution, but not on how to improve it. Improved solutions are achieved by iteratively exploring the solution space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to `Scikit-learn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The `scikit-learn` package is an open-source library that provides a robust set of machine learning algorithms for Python. It is built upon the core Python scientific stack (*i.e.* NumPy, SciPy, Cython), and has a simple, consistent interface, making it useful for many data science applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"http://1.bp.blogspot.com/-ME24ePzpzIM/UQLWTwurfXI/AAAAAAAAANw/W3EETIroA80/s1600/drop_shadows_background.png\" width=\"90%\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# previously loaded modules \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib as mplot\n",
    "%matplotlib inline\n",
    "import IPython\n",
    "from IPython.core.display import HTML\n",
    "from IPython.core.debugger import set_trace\n",
    "import xlrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# load scikit-learn modules\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random as rnd\n",
    "from random import random, randint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Representing Data in `scikit-learn`\n",
    "\n",
    "Most machine learning algorithms implemented in scikit-learn expect data to be stored in a\n",
    "**two-dimensional array or matrix**.  The arrays can be\n",
    "either ``numpy`` arrays, or in some cases ``scipy.sparse`` matrices.\n",
    "The size of the array is expected to be `[n_samples, n_features]`\n",
    "\n",
    "- **n_samples:**   The number of samples: each sample is an item to process (e.g. classify).\n",
    "  A sample can be a document, a picture, a sound, a video, an astronomical object,\n",
    "  a row in database or CSV file,\n",
    "  or whatever you can describe with a fixed set of quantitative traits.\n",
    "- **n_features:**  The number of features or distinct traits that can be used to describe each\n",
    "  item in a quantitative manner.  Features are generally real-valued, but may be boolean or\n",
    "  discrete-valued in some cases.\n",
    "\n",
    "The number of features must be fixed in advance. However it can be very high dimensional\n",
    "(e.g. millions of features) with most of them being zeros for a given sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# load data created from previous steps\n",
    "df = pd.read_pickle('./data/data_model_ready.pkl')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Most of the models in scikit-learn require the categorical variables be turned into numeric variables.  There are two approaches to this:\n",
    "1. One Hot Encoding - each item in the categorical variable is turned into its own variable represetnting the presence or abscence of that item.  For example, 'Gender' would turn into 2 variables:  'Gender_M' and 'Gender_F'\n",
    "2. Label Encoding - assign an integer to each item.  For the 'Gender' variable, 0 might mean Male and 1 might mean Female.\n",
    "\n",
    "We will use the `LabelEncoder` transformation to change categorical variables into integers.  As a convenience, we can also keep the original (human readable) variable in the Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's use the following variables as our initial set of predictors\n",
    "cat_cols = ['gender', 'marital', 'race', 'ethnicity']\n",
    "cat_cols_encoded = [c + '_encoded' for c in cat_cols]\n",
    "numeric_cols = ['prior_opioid_abuse_diag', 'age', 'opioid_discharge_days_supply']\n",
    "pred_cols = numeric_cols + cat_cols_encoded\n",
    "target_col = 'overdose'\n",
    "all_cols = cat_cols+numeric_cols+[target_col]\n",
    "\n",
    "df_opioids = df[df['prescribed_opioids'] == 1]\n",
    "\n",
    "# Encode the categorical variables\n",
    "dfe = df_opioids[cat_cols]\n",
    "\n",
    "# Replace missing data with an 'Unknown' category \n",
    "# so the missing data will also be encoded\n",
    "dfe = dfe.replace(np.NaN,'Unknown')\n",
    "\n",
    "# Encode the categorical variables\n",
    "encoded = dfe.apply(preprocessing.LabelEncoder().fit_transform)\n",
    "\n",
    "# Append the non-categorical variables and the encoded variables \n",
    "# into a single Dataframe\n",
    "# Name the new variables as <name>_encoded\n",
    "dfe = pd.concat([df_opioids[all_cols], encoded.add_suffix('_encoded')],axis=1)\n",
    "display(dfe.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# let's build a model using this set of variables...\n",
    "pred_cols = ['age', 'opioid_discharge_days_supply', \n",
    "             'prior_opioid_abuse_diag', \\\n",
    "             'gender_encoded', 'marital_encoded', 'race_encoded', \\\n",
    "             'ethnicity_encoded']\n",
    "#pred_cols = ['prior_abuse_diag', 'adult', 'age_at_visit', \\\n",
    "#             'opioid_discharge_days_supply', 'gender_encoded', \\\n",
    "#             'marital_encoded', 'race_encoded', 'ethnicity_encoded']\n",
    "\n",
    "LR_pred_cols = pred_cols\n",
    "X = dfe[pred_cols].as_matrix()\n",
    "y = dfe['overdose'].as_matrix()\n",
    "print('Using predictor variables of:',pred_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## How do we approach problems from a Data Science perspective?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Imagine a set of observational (empirical data) that we want to *learn* from...  \n",
    "<img src=\"https://www.learnopencv.com/wp-content/uploads/2017/02/data-points.png\", width='60%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can fit a variety of models ranging from extremely *simple* to highly *complex* models, e.g., \n",
    "<img src='https://www.learnopencv.com/wp-content/uploads/2017/02/bias-variance-tradeoff.png' width='60%'/>\n",
    "\n",
    "**What are potential problems with each of these cases?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "When applying these same models to a *new* set of data we held out for testing...\n",
    "<img src='https://www.learnopencv.com/wp-content/uploads/2017/02/bias-variance-tradeoff-test-error.png' width='60%'/>\n",
    "**We were overfit with the more complex, polynomial model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Bias vs. Variance\n",
    "<img src='https://www.learnopencv.com/wp-content/uploads/2017/02/Bias-Variance-Tradeoff-In-Machine-Learning-1.png' width='50%'/>\n",
    "\n",
    "**Rule of Thumb:** Fit model complexity to the data resources (not the target complexity)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Practical Approaches  \n",
    "In Data Science, we tend to do the following with a data set:  \n",
    "1. Learn a model based on training data (e.g., 60% of data)  \n",
    "2. Iteratively modify model based on a validation set:  \n",
    "    2a. Cross-validation/bootstrap with the training data  \n",
    "    2b. Separate validation set (e.g., 20% of data)  \n",
    "3. Estimate generalization error with a test set (e.g., 20% of data) that you only look at once  \n",
    "\n",
    "*Food for Thought:* With small validation sets, error measure is a bad estimator of the best hypothesis.  With large validation sets, error measure is a great estimator of a terrible hypothesis.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Create the training and test datasets\n",
    "train, test = train_test_split(dfe, test_size=0.25, random_state=123)\n",
    "\n",
    "X_train = train[pred_cols].as_matrix()\n",
    "y_train = train['overdose'].as_matrix()\n",
    "X_test = test[pred_cols].as_matrix()\n",
    "y_test = test['overdose'].as_matrix()\n",
    "\n",
    "print('X_train shape = ',X_train.shape)\n",
    "print('y_train shape = ',y_train.shape)\n",
    "print('X_test shape = ',X_test.shape)\n",
    "print('y_test shape = ',y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Let's Build Some Models!\n",
    "\n",
    "Let's begin preparing our pain data for machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# models we'll consider\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### How do these models work?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Logistic Regression\n",
    "<img src='http://www.saedsayad.com/images/LogReg_1.png' width='80%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Linear Discriminant Analysis\n",
    "<img src='http://sebastianraschka.com/images/blog/2014/linear-discriminant-analysis/lda_1.png' width='80%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### K-Nearest Neighbors\n",
    "<img src='https://upload.wikimedia.org/wikipedia/commons/thumb/e/e7/KnnClassification.svg/220px-KnnClassification.svg.png' width='30%'/>\n",
    "The test sample (green circle) should be classified either to the first class of blue squares or to the second class of red triangles. If k = 3 (solid line circle) it is assigned to the second class because there are 2 triangles and only 1 square inside the inner circle. If k = 5 (dashed line circle) it is assigned to the first class (3 squares vs. 2 triangles inside the outer circle)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Decision Trees\n",
    "<img src='https://qph.fs.quoracdn.net/main-qimg-b17755d2e0ffb326d8c39b7f3e07e03b-c' width='80%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Random Forest\n",
    "<img src='https://i.ytimg.com/vi/ajTc5y3OqSQ/hqdefault.jpg' width='65%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Gaussian Naive Bayes\n",
    "<img src='https://chrisalbon.com/images/machine_learning_flashcards/Gaussian_Naive_Bayes_Classifier_print.png' width='80%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Let's (Really) Build Some Models! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# prepare configuration for cross validation test harness \n",
    "# (from https://machinelearningmastery.com/compare-machine-learning-algorithms-python-scikit-learn/)\n",
    "seed = 123\n",
    "# prepare models\n",
    "models = []\n",
    "models.append(('LR', LogisticRegression()))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "models.append(('RF', RandomForestClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "\n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "scoring = 'roc_auc' \n",
    "# others include: 'accuracy', 'f1', 'roc_auc', \n",
    "# or found here: http://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "\n",
    "for name, model in models:\n",
    "    kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "    cv_results = model_selection.cross_val_score(model, X_train, y_train, \n",
    "                                                 cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# boxplot algorithm comparison\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "plt.ylabel(scoring)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Which model did \"best\"?  \n",
    "\n",
    "We'll tackle this more in the next section on **Model Performance and Evaluation.**  \n",
    "\n",
    "For now, just let's say *higher* is *better*.  \n",
    "\n",
    "### *Exercise:* Look at F1 scores instead of AUC scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "\n",
    "####### EDIT HERE #######\n",
    "#scoring = \n",
    "#########################\n",
    "\n",
    "for name, model in models:\n",
    "    kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "    cv_results = model_selection.cross_val_score(model, X_train, y_train, \n",
    "                                                 cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# boxplot algorithm comparison\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "plt.ylabel(scoring)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# look at the default settings you used\n",
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### *Exercise:* Attempt parameter tuning on your own\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# use Logistic Regression & Random Forests \n",
    "models = []\n",
    "\n",
    "####### EDIT HERE #######\n",
    "#models.append(('LR', LogisticRegression()))\n",
    "#models.append(('RF', RandomForestClassifier()))\n",
    "#########################\n",
    "\n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "\n",
    "####### EDIT HERE #######\n",
    "#scoring = \n",
    "#########################\n",
    "\n",
    "for name, model in models:\n",
    "    kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "    cv_results = model_selection.cross_val_score(model, X_train, y_train, \n",
    "                                                 cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### scikit-learn can help with parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "### automated grid search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = [\n",
    "        {'n_estimators': [50, 100, 250], \n",
    "         'class_weight': [None, 'balanced'], \n",
    "         'max_features': [2, 'sqrt', None]}\n",
    "]\n",
    "\n",
    "rfc = RandomForestClassifier()\n",
    "grid_search = GridSearchCV(rfc, param_grid, cv=5, scoring='f1')\n",
    "grid_search.fit(X_train, y_train)\n",
    "cvres = grid_search.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# print results\n",
    "for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
    "    print(round(np.sqrt(mean_score), 3), params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Finalizing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# assign parameters from best fit\n",
    "final_fit = RandomForestClassifier(class_weight='balanced', \n",
    "                                   max_features='sqrt', \n",
    "                                   n_estimators=50)\n",
    "\n",
    "final_fit.fit(X_train, y_train)\n",
    "\n",
    "# store predicted values using the final model\n",
    "pred_train = final_fit.predict(X_train)\n",
    "pred_test = final_fit.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# explore performance on training data\n",
    "pd.crosstab(y_train, pred_train, \n",
    "            rownames=[\"Actual\"], colnames=[\"Predicted\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# explore performance on testing data\n",
    "pd.crosstab(y_test, pred_test, \n",
    "            rownames=[\"Actual\"], colnames=[\"Predicted\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Show how to use the resulting model to predict opioid overdose\n",
    "# age, opioid_discharge_days_supply, prior_opioid_abuse_diag, \n",
    "# gender (F), marital (M), race (white), ethnicity (english)\n",
    "new_patient = [45,10,1,0,1,4,7]\n",
    "\n",
    "pred = final_fit.predict(np.asmatrix(new_patient))\n",
    "if pred[0] == 0:\n",
    "    print('Patient has no overdose risk.')\n",
    "elif pred[0] == 1:\n",
    "    print('Patient has overdose risk.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "---\n",
    "## References\n",
    "\n",
    "- [`scikit-learn` user's guide](http://scikit-learn.org/stable/user_guide.html)\n",
    "- Vanderplas, J. (2016) [Python Data Science Handbook: Essential Tools for Working with Data](http://shop.oreilly.com/product/0636920034919.do). O'Reilly Media."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  },
  "livereveal": {
   "footer": "NKBDS 2018 Pre-Conference",
   "scroll": true,
   "start_slideshow_at": "selected",
   "theme": "serif"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
